<app-tittle>Low Performance</app-tittle>

<app-paragraph>
    Machine Learning presents what is called the "Generalization Problem".
</app-paragraph>

<app-paragraph>
    The generalization problem refers to how well the concepts learned from the Machine Learning model apply to specific examples not seen by the model when it was learning.
</app-paragraph>

<app-paragraph>
    Thus, the cause of poor performance is related to this generalization problem, which can be divided into two types: Overfitting and Underfitting.
</app-paragraph>

<app-sub-title>Overfitting</app-sub-title>

<app-paragraph>
    Overfitting refers to learning the training data so well that it does not reflect the same performance/results with the new data (it does not generalize).
</app-paragraph>

<app-paragraph>
    This is the most common problem present in practice, and is more of a problem for non-parametric algorithms, which have more flexibility when learning the objective function.
</app-paragraph>

<app-paragraph>
    This type of problem can be limited with validation datasets, and by using resampling techniques.
</app-paragraph>

<app-sub-title>Underfitting</app-sub-title>

<app-paragraph>
    Underfitting refers to the failure to correctly learn the problem from the training data, so it is easy to detect.
</app-paragraph>

<app-paragraph>
    The solution to this type of problem is to alternate between the Machine Learning algorithms used.
</app-paragraph>

<app-sub-title>Sweet spot</app-sub-title>

<app-paragraph>
    Ideally, in practice, there should be a balance between overfitting and underfitting, so the sweet spot (the equilibrium point) should be taken into account.
</app-paragraph>

<app-paragraph>
    The sweet spot is the previous point where the error in the training data starts to increase if the model has good skills on the training data and the new data.
</app-paragraph>

<app-sub-title>Summary</app-sub-title>

<app-paragraph>In this section we saw:
</app-paragraph>

<ul>
    <app-list-item>
        Overfitting: Good performance on the training data, but poor generalization on the new data that is applied.
    </app-list-item>

    <app-list-item>
        Underfitting: Poor performance in the training data and poor generalization in the new data applied.
    </app-list-item>

    <app-list-item>
        Sweet spot: the optimal equilibrium point at which a learning model works most efficiently and effectively.
    </app-list-item>
</ul>